# ğŸ–¥ Processo de criaÃ§Ã£o de uma Large/Small Language Model (LLM/SLM) do zero.

Este artigo mergulha no mundo da InteligÃªncia Artificial (AI) e modelos de linguagem de grande escala (LLMs/SLMs), como o famoso GPT da OpenAI. 

LLMs/SLMs aprendem a imitar funÃ§Ãµes cognitivas humanas, identificando e replicando padrÃµes em dados de texto.  Eles utilizam uma estrutura inovadora chamada "transformers" para treinar com alta eficiÃªncia, prevendo a prÃ³xima palavra em uma sequÃªncia com precisÃ£o impressionante.

O artigo explora trÃªs tipos principais de arquiteturas Transformer:

* **Encoder:** Para tarefas de compreensÃ£o de texto (ex: BERT, RoBERTa).
* **Decoder:** Para geraÃ§Ã£o de texto (ex: GPT).
* **Encoder-Decoder:** Combinando ambas as funcionalidades (ex: T5).

AtravÃ©s de exemplo prÃ¡tico, aprenderÃ¡ a construir o seu prÃ³prio modelo de linguagem com Transformers. 

---

# ğŸ“„ PrÃ©-Processamento de Dados

O prÃ©-processamento de dados para modelos de linguagem de grande escala (LLMs/SLMs) envolve vÃ¡rias etapas detalhadas

1. Os dados sÃ£o divididos em pequenas unidades chamadas tokens, que podem ser palavras, subpalavras, nÃºmeros ou sÃ­mbolos.
2. Os dados sÃ£o limpos para remover erros, conteÃºdo ofensivo ou spam. O texto Ã© entÃ£o normalizado, o que pode incluir a conversÃ£o de todas as letras para minÃºsculas, remoÃ§Ã£o de stopwords, e aplicaÃ§Ã£o de tÃ©cnicas de stemming ou lematizaÃ§Ã£o.
3. Esses 'tokens' sÃ£o transformados em nÃºmeros atravÃ©s de tÃ©cnicas como embedding, para que o modelo possa processÃ¡-los eficientemente.

> [!NOTE]
> Um **token** Ã© uma unidade bÃ¡sica de linguagem em um modelo de linguagem geradora, como GPT-2 ou BERT. Ã‰ uma palavra ou um sÃ­mbolo que representa uma ideia ou uma informaÃ§Ã£o especÃ­fica. Por exemplo, "O gato" Ã© um token, e "gato" Ã© um subtoken.

> [!NOTE]
> Uma tÃ©cnica de **embedding** Ã© uma abordagem utilizada em machine learning para mapear objetos de diferentes domÃ­nios para um espaÃ§o vetorial comum. Em outras palavras, ela permite que os modelos de linguagem geradora representem informaÃ§Ãµes de diferentes fontes de dados em um formato comum e fÃ¡cil de processar.

Lembre-se de que a criaÃ§Ã£o de um **dataset** Ã© uma tarefa desafiadora e requer tempo e esforÃ§o. No entanto, com certeza e dedicaÃ§Ã£o, vocÃª pode criar um dataset Ãºtil para o treinamento do modelo de linguagem ou para outras tarefas especÃ­ficas.
Um **dataset** pode ser uma forma de tabela de perguntas e respostas e em forma JSON e deverÃ¡ ser em inglÃªs para obter melhores resultados.

Verifica se jÃ¡ nÃ£o hÃ¡ um dataset que possas melhorar e usar, https://huggingface.co/datasets

ğŸ‘‰ [YouTube](https://www.youtube.com/results?search_query=como+fazer+datasets+para+llm) mais videos sobre datasets

Exemplo ficheiro json com tema de futebol:
```
[
{
    "question": "What are the dimensions of a regulation soccer field?",
    "answer": "A regulation soccer field is 100-130 yards long and 50-100 yards wide."
  },
  {
    "question": "How many players are on the field for each team in a soccer match?",
    "answer": "Each team has 11 players on the field at a time, including the goalkeeper."
  },
  {
    "question": "What is an offside rule in soccer?",
    "answer": "A player is offside if they are closer to the opponent's goal line than both the ball and the second-to-last defender when the ball is played to them."
  },
  {
    "question": "What is a penalty kick in soccer?",
    "answer": "A penalty kick is awarded to the attacking team when a foul is committed by a defender inside the penalty area."
  },
  {
    "question": "What are the different positions in a soccer team?",
    "answer": "Common positions include goalkeeper, defenders (center-backs, full-backs), midfielders (central midfielders, wingers), and forwards (strikers)."
  }
]
```

---

# ğŸ§  Machine Learning

Para treinar um LLM/SLM, Ã© necessÃ¡rio equipamento especializado, como GPUs ou TPUs de alto desempenho, para processar grandes volumes de dados e realizar cÃ¡lculos complexos. Neste exemplo, utilizarei o **Google Colab**, que oferece acesso gratuito a GPUs e TPUs , possibilitando um treinamento sem a necessidade de investir em hardware caro.

> Um arquivo `.ipynb` no Google Colab Ã© um arquivo de notebook Jupyter. O Google Colab Ã© uma plataforma gratuita e em nuvem que permite a execuÃ§Ã£o de cÃ³digo Python, R e Julia em um ambiente de notebook interativo. Os arquivos `.ipynb` sÃ£o usados para armazenar e compartilhar cÃ³digos Jupyter, que sÃ£o documentos que contÃªm cÃ³digo, texto e visualizaÃ§Ãµes.

## ğŸ”¨ Crie um Google Colab e habilite a T4 GPU

Aqui estÃ£o os passos para criar um Google Colab e habilitar a GPU T4:

### 1. Acesse o Google Colab:

* VÃ¡ para o site do Google Colab: [https://colab.research.google.com/](https://colab.research.google.com/)

### 2. Crie um novo Notebook:

* Clique no botÃ£o "Novo" para criar um novo notebook.

### 3. Habilite a GPU T4:

* **Clique no menu "Runtime" no topo da tela.**
* **Selecione "Change runtime type".**
* **Na janela que aparece, escolha "GPU" como o "Hardware accelerator".**
* **Selecione "Tesla T4" como o tipo de GPU.**
* **Clique em "Save".**

### 4. Verifique se a GPU estÃ¡ habilitada:

* Execute o seguinte cÃ³digo no seu notebook:

```python
  !nvidia-smi
```

* Isso exibirÃ¡ informaÃ§Ãµes sobre a GPU T4 habilitada, incluindo o nome do modelo, memÃ³ria disponÃ­vel e uso da GPU.

**ObservaÃ§Ãµes:**

* A disponibilidade de GPUs T4 pode variar dependendo da demanda. Se vocÃª nÃ£o conseguir encontrar a opÃ§Ã£o T4, tente escolher outra GPU disponÃ­vel, como a Tesla K80.
* O uso de GPUs pode resultar em custos adicionais, dependendo do tempo de execuÃ§Ã£o e da quantidade de recursos utilizados.


## ğŸ”¨ InstalaÃ§Ã£o das bibliotecas necessÃ¡rias

```batch
	pip install transformers[torch] datasets torch
```

A biblioteca transformers da Hugging Face oferece modelos de linguagem prÃ©-treinados como GPT-2, BERT e T5 para tarefas como geraÃ§Ã£o de texto, resposta a perguntas e traduÃ§Ã£o. Ã‰ fÃ¡cil de usar e permite treinar ou ajustar modelos para necessidades especÃ­ficas. Ã‰ uma ferramenta essencial para aplicar inteligÃªncia artificial em linguagem natural. A biblioteca torch Ã© usada para computaÃ§Ã£o e treinamento eficiente, e a biblioteca datasets Ã© usada para manipular nossos dados de treino.

Aqui estÃ¡ um resumo dos modelos mais populares da biblioteca Transformers da Hugging Face:

1. BERT (Bidirectional Encoder Representations from Transformers)
	- **DescriÃ§Ã£o**: Um modelo prÃ©-treinado bidirecional para NLP, especializado em compreensÃ£o contextual.
	- **Usos**: ClassificaÃ§Ã£o de texto, resposta a perguntas, reconhecimento de entidades nomeadas.
	- **Exemplo**: `bert-base-uncased`

2. GPT (Generative Pre-trained Transformer)
	- **DescriÃ§Ã£o**: Um modelo de geraÃ§Ã£o de texto autoregressivo.
	- **Usos**: GeraÃ§Ã£o de texto, completamento de texto, chatbots.
	- **Exemplo**: `gpt2`, `gpt-3.5-turbo`

3. RoBERTa (A Robustly Optimized BERT Pretraining Approach)
	- **DescriÃ§Ã£o**: Uma variante de BERT otimizada com mais dados e ajustes de hiperparÃ¢metros.
	- **Usos**: Tarefas semelhantes ao BERT, com melhor desempenho.
	- **Exemplo**: `roberta-base`

4. DistilBERT (Distilled version of BERT)
	- **DescriÃ§Ã£o**: Uma versÃ£o compacta de BERT que Ã© mais rÃ¡pida e eficiente.
	- **Usos**: Tarefas de NLP com menor necessidade computacional.
	- **Exemplo**: `distilbert-base-uncased`

5. T5 (Text-To-Text Transfer Transformer)
	- **DescriÃ§Ã£o**: Um modelo que trata todas as tarefas de NLP como problemas de traduÃ§Ã£o de texto-para-texto.
	- **Usos**: TraduÃ§Ã£o, sumarizaÃ§Ã£o, geraÃ§Ã£o de texto.
	- **Exemplo**: `t5-small`, `t5-base`

6. XLNet
	- **DescriÃ§Ã£o**: Um modelo autoregressivo que tambÃ©m captura dependÃªncias bidirecionais.
	- **Usos**: Modelos de linguagem com prediÃ§Ã£o bidirecional.
	- **Exemplo**: `xlnet-base-cased`

7. ALBERT (A Lite BERT)
	- **DescriÃ§Ã£o**: Uma versÃ£o leve de BERT com menos parÃ¢metros e arquitetura eficiente.
	- **Usos**: Tarefas de NLP com menor consumo de memÃ³ria e mais rÃ¡pidas.
	- **Exemplo**: `albert-base-v2`

8. Bart (Bidirectional and Auto-Regressive Transformers)
	- **DescriÃ§Ã£o**: Um modelo que combina as capacidades de modelos bidirecionais e autoregressivos.
	- **Usos**: TraduÃ§Ã£o, sumarizaÃ§Ã£o, geraÃ§Ã£o de texto.
	- **Exemplo**: `facebook/bart-large`

9. Electra (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)
	- **DescriÃ§Ã£o**: Um modelo eficiente que aprende a discriminar entre tokens reais e substituÃ­dos.
	- **Usos**: PrÃ©-treino eficiente para tarefas de NLP.
	- **Exemplo**: `google/electra-small-discriminator`

10. BERTweet
	- **DescriÃ§Ã£o**: Um modelo baseado em BERT treinado especificamente em dados do Twitter.
	- **Usos**: AnÃ¡lise de sentimentos, classificaÃ§Ã£o de texto em tweets.
	- **Exemplo**: `vinai/bertweet-base`

## ğŸ”¨ ImportaÃ§Ã£o das bibliotecas

ImportaÃ§Ã£o das bibliotecas
```python
	  from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
	  from datasets import Dataset
	  import json
	  import pandas as pd
```

[LINK](https://huggingface.co/docs/transformers/v4.43.3/en/model_doc/gpt2)

Neste passo, importamos as bibliotecas essenciais para manipulaÃ§Ã£o de dados e deep learning. Utilizamos o torch para operaÃ§Ãµes de tensor e computaÃ§Ã£o em GPU.
A biblioteca transformers para carregar e treinar o modelo GPT-2, incluindo GPT2Tokenizer, GPT2LMHeadModel, Trainer e TrainingArguments.
O datasets do Hugging Face para preparar e manipular conjuntos de dados.
A biblioteca padrÃ£o json para leitura de arquivos JSON que Ã© o nosso ficheiro de dados, e o pandas para transformar dados JSON em DataFrames para anÃ¡lise e manipulaÃ§Ã£o eficientes. Essas bibliotecas sÃ£o fundamentais para realizar tarefas de processamento de linguagem natural (NLP) de maneira eficaz.

## ğŸ”¨ Carregar dados do arquivo JSON
```python
	  def load_data(file_path):
	    with open(file_path, 'r') as file:
	    data = json.load(file)
	    return pd.DataFrame(data)
	  
	  file_path = '/content/dataset.json'
	  df = load_data(file_path)
```

No colab ao lado esquerdo folder icon, '/content' Ã© o root, carrega o ficheiro para lÃ¡!

O objetivo desta etapa Ã© carregar os dados contidos em um arquivo JSON e convertÃª-los em um DataFrame do Pandas. 
Um DataFrame Ã© uma estrutura de dados bidimensional que facilita a manipulaÃ§Ã£o e anÃ¡lise dos dados.
A CriaÃ§Ã£o do JSON que contem perguntas e respostas podes criar usando uma AI. Utiliza uma sÃ©rie de prompts para gerar JSONs contendo perguntas e respostas sobre uma ampla gama de tÃ³picos que desejas treinar o teu modelo. 
Para garantir a diversidade das informaÃ§Ãµes, usa [crewAI](https://www.crewai.com/) onde terÃ¡s ligaÃ§Ã£o a modelos OpenGPT, Gemini, Claude etc
Verifica depois se tens ou nÃ£o perguntas repetidas! Ã‰ muito importante nÃ£o ter perguntas repetidas. Possivelmente terÃ¡s de criar um cÃ³digo/script que faÃ§a isso automaticamente.
Nesta fase Ã© onde terÃ¡s de ler e aprender a ter os dados necessÃ¡rios e correctos. O Fine Tuning usarÃ¡ tambÃ©m datasets!

## ğŸ”¨ Inicializar tokenizador e preparar dados  

```python
	tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')
	tokenizer.pad_token = tokenizer.eos_token

	df['text'] = df['question'] + " " + df['answer']

	def tokenize_function(examples):
    		return tokenizer(examples, padding="max_length", truncation=True, max_length=512)

	tokenized_datasets = df['text'].apply(lambda x: tokenize_function(x))
	dataset = Dataset.from_pandas(df)

	def tokenize_dataset(dataset):
    		return dataset.map(lambda x: tokenizer(x['text'], padding="max_length", truncation=True, max_length=512), batched=True, remove_columns=["text", "question", "answer"])

	tokenized_dataset = tokenize_dataset(dataset)
```

A biblioteca Transformers da Hugging Face oferece uma ampla variedade de modelos prÃ©-treinados que vocÃª pode utilizar com o `GPT2Tokenizer.from_pretrained`. Aqui estÃ£o **algumas** opÃ§Ãµes:

1. **`gpt2`** : O modelo original GPT-2, treinado em um conjunto de dados de 45 GB de texto.
2. **`gpt2-medium`** : Uma versÃ£o mÃ©dia do modelo GPT-2, treinada em um conjunto de dados de 10 GB de texto.
3. **`gpt2-large`** : Uma versÃ£o grande do modelo GPT-2, treinada em um conjunto de dados de 300 GB de texto.
4. **`gpt2-xl`** : Uma versÃ£o extra grande do modelo GPT-2, treinada em um conjunto de dados de 1,2 TB de texto.
5. **`gpt2-125M`** : Uma versÃ£o do modelo GPT-2 com 125 milhÃµes de parÃ¢metros, treinada em um conjunto de dados de 10 GB de texto.
6. **`gpt2-355M`** : Uma versÃ£o do modelo GPT-2 com 355 milhÃµes de parÃ¢metros, treinada em um conjunto de dados de 10 GB de texto.
7. **`gpt2-774M`** : Uma versÃ£o do modelo GPT-2 com 774 milhÃµes de parÃ¢metros, treinada em um conjunto de dados de 10 GB de texto.
8. **`gpt2-125M-uncased`** : Uma versÃ£o do modelo GPT-2 com 125 milhÃµes de parÃ¢metros, treinada em um conjunto de dados de 10 GB de texto e sem case sensitivity.
9. **`gpt2-355M-uncased`** : Uma versÃ£o do modelo GPT-2 com 355 milhÃµes de parÃ¢metros, treinada em um conjunto de dados de 10 GB de texto e sem case sensitivity.
10. **`gpt2-774M-uncased`** : Uma versÃ£o do modelo GPT-2 com 774 milhÃµes de parÃ¢metros, treinada em um conjunto de dados de 10 GB de texto e sem case sensitivity.

Lembre-se de que cada modelo prÃ©-treinado tem suas prÃ³prias caracterÃ­sticas e habilidades, e vocÃª deve escolher o modelo que melhor se adequare Ã s suas necessidades especÃ­ficas.

**[distilgpt2](https://huggingface.co/distilbert/distilgpt2)** Ã© um modelo de lÃ­ngua **inglesa prÃ©-treinado** gerador de texto baseado no modelo GPT-2 (Generative Pre-trained Transformer 2), que Ã© uma das melhores opÃ§Ãµes para a geraÃ§Ã£o de texto natural. Ele foi projetado para ser mais eficiente e fÃ¡cil de usar do que o modelo original, tornando-o uma opÃ§Ã£o popular entre os desenvolvedores que buscam treinar modelos de linguagem Decoders (GPT).

Neste passo, inicializamos o tokenizador GPT2Tokenizer do modelo **distilgpt2** e configuramos o token de padding para ser o mesmo que o token de fim de sequÃªncia (EOS). Concatenamos perguntas e respostas em uma nova coluna text no DataFrame e definimos uma funÃ§Ã£o de tokenizaÃ§Ã£o que aplica padding e truncamento atÃ© um comprimento mÃ¡ximo de 512 tokens. Aplicamos esta funÃ§Ã£o de tokenizaÃ§Ã£o a cada texto no DataFrame, convertendo-o em tokens. Em seguida, transformamos o DataFrame em um Dataset do Hugging Face, facilitando o processamento eficiente e removendo colunas originais apÃ³s a tokenizaÃ§Ã£o para otimizaÃ§Ã£o.
O token de padding Ã© utilizado para garantir que todas as sequÃªncias de entrada em um lote (batch) de dados tenham o mesmo comprimento. Isso Ã© necessÃ¡rio porque os modelos de deep learning, como os Transformers, requerem que as entradas tenham dimensÃµes consistentes para processamento eficiente em paralelo.

## ğŸ”¨ Adicionar labels e dividir dataset

codigo completo :
```python
	tokenized_dataset = tokenized_dataset.map(lambda examples: {'labels': examples['input_ids']}, batched=True)

	train_test_split = tokenized_dataset.train_test_split(test_size=0.15)
	train_dataset = train_test_split['train']
	test_dataset = train_test_split['test']
```

## ğŸ”¨ ExplicaÃ§Ã£o detalhada da etapa de adiÃ§Ã£o de labels e divisÃ£o do dataset:

Este passo Ã© fundamental para preparar os dados para o treinamento do modelo de linguagem. 

### 1. Adicionando Labels:

```python
tokenized_dataset = tokenized_dataset.map(lambda examples: {'labels': examples['input_ids']}, batched=True)
```

* **Objetivo:** Atribuir labels aos dados tokenizados. 
* **Como funciona:**
    * `tokenized_dataset.map`: Aplica uma funÃ§Ã£o a cada exemplo do dataset.
    * `lambda examples: {'labels': examples['input_ids']}`:  FunÃ§Ã£o que cria um novo dicionÃ¡rio para cada exemplo, adicionando uma chave 'labels' com o valor de 'input_ids'.  Os 'input_ids' representam as representaÃ§Ãµes numÃ©ricas das palavras no texto, que serÃ£o usadas como labels para o modelo aprender a prever as prÃ³ximas palavras.
    * `batched=True`: Processa os exemplos em batches para otimizar o desempenho.

### 2. Dividindo o Dataset:

```python
train_test_split = tokenized_dataset.train_test_split(test_size=0.15)
train_dataset = train_test_split['train']
test_dataset = train_test_split['test']
```

* **Objetivo:** Separar o dataset em conjuntos de treino e teste.
* **Como funciona:**
    * `tokenized_dataset.train_test_split(test_size=0.15)`: Divide o dataset em 85% para treino e 15% para teste.
    * `train_dataset = train_test_split['train']`: Atribui o conjunto de treino Ã  variÃ¡vel `train_dataset`.
    * `test_dataset = train_test_split['test']`: Atribui o conjunto de teste Ã  variÃ¡vel `test_dataset`.

**ImportÃ¢ncia da DivisÃ£o:**

* **PrevenÃ§Ã£o de Overfitting:** O modelo aprende melhor com dados que ele nÃ£o viu durante o treinamento. O conjunto de teste garante que o modelo seja avaliado com dados nÃ£o vistos, evitando que ele memorize o conjunto de treino e se torne incapaz de generalizar para novos dados.
* **AvaliaÃ§Ã£o do Desempenho:** O conjunto de teste permite avaliar o desempenho do modelo em dados reais e comparar diferentes modelos.


## ğŸ”¨ FunÃ§Ã£o de agrupamento de dados

```python
	def data_collator(features):
    		batch = {}
    		batch['input_ids'] = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)
    		batch['attention_mask'] = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)
    		batch['labels'] = torch.tensor([f['labels'] for f in features], dtype=torch.long)
    		return batch
```

## ğŸ”¨ A funÃ§Ã£o `data_collator`:  Organizando os dados para o treinamento eficiente

A funÃ§Ã£o `data_collator` organiza dados tokenizados em batches para treinamento eficiente de modelos de linguagem. Ela converte listas de tokens, mÃ¡scaras de atenÃ§Ã£o e rÃ³tulos em tensores PyTorch, garantindo que todos os batches tenham a mesma estrutura atravÃ©s de tÃ©cnicas de padding. Essa organizaÃ§Ã£o facilita o processamento computacional, garante consistÃªncia no treinamento e permite uma integraÃ§Ã£o simplificada com o Hugging Face Trainer. 

> [!NOTE]
> Tensores PyTorch sÃ£o estruturas de dados fundamentais similares aos arrays multidimensionais mas com a vantagem adicional de serem otimizados para operaÃ§Ãµes de alto desempenho em GPUs (unidades de processamento grÃ¡fico).

## ğŸ”¨ Inicializar modelo e argumentos de treinamento

```python
	model = GPT2LMHeadModel.from_pretrained('distilgpt2')
	
	training_args = TrainingArguments(
	    output_dir='./results',
	    eval_strategy="epoch",
	    learning_rate=3e-5,
	    per_device_train_batch_size=8,
	    per_device_eval_batch_size=8,
	    num_train_epochs=5,
	    weight_decay=0.01,
	    logging_dir='./logs',
	    logging_steps=10,
	)
```

Neste passo, preparamos o ambiente para o treinamento do modelo GPT-2. 

Quando vocÃª executa o comando `model = GPT2LMHeadModel.from_pretrained('distilgpt2')`, vocÃª estÃ¡ carregando o modelo de linguagem GPT-2, que Ã© uma rede neural treinada para prever as prÃ³ximas palavras em uma sequÃªncia de texto.

Aqui estÃ£o alguns dos parÃ¢metros do modelo GPT-2:

* **NÃºmero de camadas**: 12
* **NÃºmero de neurÃ´nios por camada**: 768
* **Tamanho da janela de atenÃ§Ã£o**: 512
* **NÃºmero de heads de atenÃ§Ã£o**: 12
* **Taxa de aprendizado**: 1e-4
* **NÃºmero de Ã©pocas de treinamento**: 30
* **Tamanho do batch**: 32
* **RegularizaÃ§Ã£o**: L2 regularization com um peso de 0.1
* **Dropout**: 0.1

Ã‰ importante notar que esses parÃ¢metros sÃ£o os padrÃµes utilizados durante o treinamento do modelo GPT-2. VocÃª pode ajustar esses parÃ¢metros para melhorar o desempenho do modelo em seu problema especÃ­fico.

Se vocÃª quiser saber mais sobre os parÃ¢metros do modelo GPT-2, recomendo que vocÃª leia o artigo original sobre o modelo GPT-2, publicado na arXiv em 2019.

* **Configuramos o treinamento:** Definimos os parÃ¢metros de treinamento usando `TrainingArguments`, controlando aspectos como:
    * **SaÃ­da:** DiretÃ³rio para salvar os resultados do treinamento.
    * **AvaliaÃ§Ã£o:** EstratÃ©gia para avaliar o desempenho do modelo.
    * **Taxa de aprendizado:**  Velocidade de ajuste dos parÃ¢metros do modelo.
    * **Tamanho do batch:** Quantidade de dados processados por vez.
    * **NÃºmero de passagens:**  NÃºmero de vezes que todo o dataset Ã© usado para treinamento.
    * **Decaimento de peso:**  ReduÃ§Ã£o gradual da taxa de aprendisagem durante o treinamento.
    * **Logs:** DiretÃ³rio para salvar informaÃ§Ãµes sobre o progresso do treinamento.
    * **FrequÃªncia de registro:**  Intervalo de tempo para registrar informaÃ§Ãµes sobre o treinamento.

Alguns frameworks de machine learning, como TensorFlow e PyTorch, oferecem suporte Ã  **quantizaÃ§Ã£o** de pesos e ativamentos em modelos de Transformer.
A quantizaÃ§Ã£o Ã© uma tÃ©cnica usada para reduzir a precisÃ£o dos nÃºmeros que representam os pesos e ativamentos de uma rede neural. Em vez de usar nÃºmeros de 32 bits (como Ã© comum em muitas implementaÃ§Ãµes), os valores sÃ£o representados com menos bits, como 8 bits ou atÃ© 4 bits. Isso tem vÃ¡rias vantagens:

1. ReduÃ§Ã£o do tamanho do modelo: Um modelo quantizado ocupa menos espaÃ§o na memÃ³ria, o que Ã© Ãºtil para implementaÃ§Ãµes em dispositivos com recursos limitados, como smartphones ou dispositivos IoT.
2. Melhoria na velocidade de inferÃªncia: OperaÃ§Ãµes com nÃºmeros de menor precisÃ£o podem ser computadas mais rapidamente, o que acelera o tempo de inferÃªncia.
3. Menor consumo de energia: Modelos quantizados consomem menos energia, o que Ã© importante para dispositivos mÃ³veis e outros sistemas embarcados.

No Hugging Face e outras plataformas de modelagem, vocÃª pode ver modelos quantizados em 8 bits ou 4 bits usados para tarefas como processamento de linguagem natural, visÃ£o computacional, entre outros. Estes modelos sÃ£o frequentemente utilizados em cenÃ¡rios onde a eficiÃªncia e a velocidade sÃ£o cruciais, e onde os dispositivos de execuÃ§Ã£o tÃªm limitaÃ§Ãµes de memÃ³ria e poder de processamento.

* **Iniciando o treinamento:**  O `Trainer` utiliza esses argumentos, juntamente com os datasets e a funÃ§Ã£o de agrupamento de dados (`data_collator`), para gerenciar o processo de treinamento e avaliaÃ§Ã£o do modelo de forma eficiente e controlada.

## ğŸ”¨ Configurar o trainer e iniciar o treinamento

```python
	trainer = Trainer(
	    model=model,
	    args=training_args,
	    train_dataset=train_dataset,
	    eval_dataset=test_dataset,
	    data_collator=data_collator
	)
	
	trainer.train()
```

Este passo envolve a configuraÃ§Ã£o do objeto Trainer da biblioteca transformers.

   - model=model: Especifica o modelo de aprendizado de mÃ¡quina que serÃ¡ treinado. Esse modelo jÃ¡ deve estar previamente carregado e configurado que serÃ¡ **distilgpt2**
   - args=training_args: ConfiguraÃ§Ãµes e parÃ¢metros de treinamento, geralmente um objeto da classe TrainingArguments. Isso pode incluir informaÃ§Ãµes como nÃºmero de Ã©pocas, tamanhos de lote (batch size), taxas de aprendizado e dispositivos a serem usados (CPU/GPU).
   - train_dataset=train_dataset: O conjunto de dados que serÃ¡ usado para treinar o modelo.
   - eval_dataset=test_dataset: O conjunto de dados que serÃ¡ usado para avaliar o desempenho do modelo durante o treinamento.
   - data_collator=data_collator: Um objeto que identifica como os dados devem ser agrupados em lotes (batches) durante o treinamento e a avaliaÃ§Ã£o.

ApÃ³s a configuraÃ§Ã£o, o treinamento Ã© iniciado com `trainer.train()`.

Etapas de treino sÃ£o forward pass >> cÃ¡lculo da perda >> backward pass >> atualizaÃ§Ã£o dos parÃ¢metros

> [!NOTE]
> O forward pass Ã© a etapa em que os dados de entrada sÃ£o passados pela rede neural, camada por camada, atÃ© que uma previsÃ£o (ou saÃ­da) seja gerada. Ele transforma inputs em outputs. Imagine que vocÃª estÃ¡ fornecendo ao modelo uma frase, como "O gato estÃ¡ dormindo". O modelo lÃª a frase e tenta prever a prÃ³xima palavra na frase, com base nas palavras que viu antes. Isso Ã© chamado de **forward pass** porque o modelo estÃ¡ se movendo para frente, processando a frase de entrada e fazendo previsÃµes.

> [!NOTE]
> O cÃ¡lculo da perda quantifica o erro das prediÃ§Ãµes da rede comparado aos valores reais, utilizando funÃ§Ãµes de perda especÃ­ficas. Este valor Ã© crucial para ajustar os pesos da rede e melhorar a precisÃ£o do modelo, mede o quÃ£o distante as prediÃ§Ãµes da rede estÃ£o dos valores reais. Se o modelo prevÃª a palavra correta, a perda Ã© baixa. Se ele prevÃª uma palavra errada, a perda Ã© alta. O objetivo Ã© minimizar a perda, o que significa que o modelo estÃ¡ melhorando para prever a prÃ³xima palavra.

> [!NOTE]
> O backward pass Ã© um passo importante no treinamento de modelos de inteligÃªncia artificial. Nesse passo, **backward pass** Ã© o oposto do **forward pass**. Em vez de se mover para frente, o modelo se move para trÃ¡s, ajustando seus parÃ¢metros internos para reduzir a perda. Isso Ã© como o modelo dizendo: "Ah, eu errei! Vou tentar novamente e farei melhor!"

> [!NOTE]
> No AtualizaÃ§Ã£o dos ParÃ¢metros durante o backward pass, o modelo atualiza seus parÃ¢metros internos, como pesos e bias de sua rede neural. Esses parÃ¢metros sÃ£o ajustados com base na diferenÃ§a entre a saÃ­da prevista e a saÃ­da real. O objetivo Ã© encontrar o conjunto Ã³timo de parÃ¢metros que minimize a perda.

ApÃ³s o terminus do treinamento irÃ¡ aparecer na consola:
```
	TrainOutput(global_step=800, training_loss=0.2628884120285511, 
	metrics={'train_runtime': 692.0873, 'train_samples_per_second': 9.211, 
	'train_steps_per_second': 1.156, 'total_flos': 832883392512000.0, 
	'train_loss': 0.2628884120285511, 'epoch': 5.0})
```
Ele fornece informaÃ§Ãµes sobre o estado do treinamento, incluindo:

* O passo global do treinamento (global_step): 800
* A perda de treinamento (training_loss): 0.2628884120285511
* MÃ©tricas de desempenho do treinamento, incluindo:
	+ Tempo de execuÃ§Ã£o do treinamento (train_runtime): 692.0873 segundos
	+ NÃºmero de amostras por segundo (train_samples_per_second): 9.211
	+ NÃºmero de passos por segundo (train_steps_per_second): 1.156
	+ NÃºmero total de operaÃ§Ãµes floating-point (total_flos): 832883392512000.0
	+ Perda de treinamento (train_loss): 0.2628884120285511
	+ Ã‰poca atual (epoch): 5.0

Essas informaÃ§Ãµes podem ser Ãºteis para monitorar o progresso do treinamento e ajustar os parÃ¢metros do modelo para melhorar o desempenho.

## ğŸ”¨ Salvar o modelo e tokenizador treinados

```python
	model.save_pretrained("./gpt2-chatbot")
	tokenizer.save_pretrained("./gpt2-chatbot")
```

ApÃ³s o treinamento do modelo, Ã© crucial salvar tanto o modelo quanto o tokenizador para reutilizaÃ§Ã£o futura. Isso Ã© feito utilizando os mÃ©todos save_pretrained do GPT2LMHeadModel e GPT2Tokenizer, que armazenam os pesos treinados, configuraÃ§Ãµes, e vocabulÃ¡rio em um diretÃ³rio especificado, como ./gpt2-chatbot. Salvar esses componentes permite carregÃ¡-los posteriormente com from_pretrained, evitando a necessidade de retraining, facilitando o compartilhamento, backup e versionamento do modelo, garantindo eficiÃªncia e consistÃªncia nas inferÃªncias futuras.

## ğŸ”¨ Carregar modelo e tokenizador treinados

```python
	model = GPT2LMHeadModel.from_pretrained("./gpt2-chatbot")
	tokenizer = GPT2Tokenizer.from_pretrained("./gpt2-chatbot")
```

Este passo envolve carregar o modelo e o tokenizador treinados usando a funÃ§Ã£o from_pretrained(), que permite reutilizar os pesos ajustados do modelo GPT-2 e as configuraÃ§Ãµes do tokenizador sem precisar retrainar. Isso Ã© essencial para aplicaÃ§Ãµes prÃ¡ticas, como responder perguntas dos usuÃ¡rios em um aplicativo web. O cÃ³digo carrega o modelo e o tokenizador do diretÃ³rio ./gpt2-chatbot, e inclui uma funÃ§Ã£o gerar_resposta (passo 12) que tokeniza a entrada do usuÃ¡rio, gera uma resposta com o modelo, e decodifica a saÃ­da para texto. Este processo garante inferÃªncias rÃ¡pidas e consistentes com o treinamento.

## ğŸ”¨ FunÃ§Ã£o para gerar resposta

```python
	def gerar_resposta(model, tokenizer, input_text, max_length=50, num_return_sequences=1):
	    inputs = tokenizer.encode(input_text, return_tensors='pt')
	    attention_mask = [1] * len(inputs[0])
	    outputs = model.generate(inputs, attention_mask=torch.tensor([attention_mask]), max_length=max_length, num_return_sequences=num_return_sequences, pad_token_id=tokenizer.eos_token_id)
	    generated_text = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]
	    return generated_text
```

Esta Ã© uma funÃ§Ã£o Python que gera uma resposta baseada em um modelo de linguagem treinado. A funÃ§Ã£o Ã© chamada `gerar_resposta` e tem quatro parÃ¢metros:

* `model`: um modelo de linguagem treinado
* `tokenizer`: um objeto que Ã© usado para codificar e decodificar texto
* `input_text`: o texto de entrada que serÃ¡ usado para gerar a resposta
* `max_length` (opcional): o tamanho mÃ¡ximo da resposta gerada (padrÃ£o Ã© 50)
* `num_return_sequences` (opcional): o nÃºmero de respostas geradas (padrÃ£o Ã© 1)

Aqui estÃ¡ o que a funÃ§Ã£o faz:

1. Codifica o texto de entrada usando o objeto `tokenizer` e armazena o resultado em uma variÃ¡vel chamada `inputs`.
2. Cria uma mÃ¡scara de atenÃ§Ã£o* que Ã© usada para indicar quais tokens do texto de entrada devem ser considerados quando o modelo gera a resposta.
3. Chama o mÃ©todo `generate` do modelo para gerar a resposta. O mÃ©todo `generate` Ã© usado para gerar texto baseado em um texto de entrada e um modelo de linguagem.
4. O mÃ©todo `generate` retorna uma lista de saÃ­das, que sÃ£o as respostas geradas. A funÃ§Ã£o itera sobre essa lista e decodifica cada saÃ­da usando o objeto `tokenizer`.
5. A funÃ§Ã£o remove os tokens especiais (como tokens de inÃ­cio e fim de texto) da resposta gerada usando o mÃ©todo `decode` do objeto `tokenizer`.
6. A funÃ§Ã£o retorna a lista de respostas geradas.

Em resumo, esta funÃ§Ã£o Ã© usada para gerar respostas baseadas em um modelo de linguagem treinado, com base em um texto de entrada.

> [!NOTE]
> MÃ¡scara de atenÃ§Ã£o* Ã© uma ferramenta usada em modelos de linguagem, especialmente em arquiteturas de transformadores, como GPT-2, para controlar quais tokens (palavras ou sub-palavras) em uma sequÃªncia de entrada devem ser considerados (ou â€œatendidosâ€) pelo modelo em diferentes etapas de processamento.


## ğŸ”¨ Exemplo de uso da funÃ§Ã£o de geraÃ§Ã£o de resposta

```python
	input_text = "What is the term for a foul committed by a player that prevents an opponent from scoring a goal?"
	resposta = gerar_resposta(model, tokenizer, input_text)
	print(f"{resposta}")
```

Essa funÃ§Ã£o Ã© Ãºtil para criar chatbots e sistemas de resposta automÃ¡tica que necessitam de geraÃ§Ã£o de texto baseado em modelos de linguagem.







